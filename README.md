# *OBS: OS NOMES DOS CAMPOS FORAM OMITIDOS POR QUESTÃ•ES DE LGPD*

# **ğŸš€ Validador de Dados para AtualizaÃ§Ã£o de Dashboards**  

## ğŸ“Œ **Sobre o Projeto**  
Este repositÃ³rio apresenta um **validador automÃ¡tico** desenvolvido para ser utilizado durante os processos de **atualizaÃ§Ã£o de dashboards**. O objetivo Ã© **detectar variaÃ§Ãµes anÃ´malas (outliers) e reportÃ¡-las via Microsoft Teams antes da finalizaÃ§Ã£o do processo**, evitando retrabalho e inconsistÃªncias nos relatÃ³rios.

## ğŸ¯ **Objetivo**  
âœ”ï¸ Validar os dados antes da atualizaÃ§Ã£o dos dashboards  
âœ”ï¸ Detectar **outliers** na contagem de clientes e rentabilidade  
âœ”ï¸ Alertar a equipe via **Microsoft Teams** em caso de inconsistÃªncias  
âœ”ï¸ Reduzir retrabalho e garantir maior confiabilidade nos dashboards  

## âš™ï¸ **Funcionamento**  
1ï¸âƒ£ **Consulta os dados** no ambiente PySpark SQL com filtros especÃ­ficos.  
2ï¸âƒ£ **Verifica a contagem de registros** e identifica possÃ­veis anomalias.  
3ï¸âƒ£ **Gera uma mensagem de monitoramento**, diferenciando casos normais e crÃ­ticos.  
4ï¸âƒ£ **Envia um alerta para o Microsoft Teams**, permitindo a intervenÃ§Ã£o antes da conclusÃ£o da atualizaÃ§Ã£o.  

## ğŸ›  **Tecnologias Utilizadas**  
- **Linguagens:** Python, SQL, PySpark  
- **Big Data:** Databricks, SparkSQL  
- **NotificaÃ§Ã£o:** pymsteams (Microsoft Teams Webhook)  
- **ManipulaÃ§Ã£o de Dados:** Pandas, PySpark DataFrames

## ğŸ“Œ **BenefÃ­cios do Validador**  
âœ… **AutomaÃ§Ã£o do processo de validaÃ§Ã£o**, evitando inconsistÃªncias nos dashboards.  
âœ… **IdentificaÃ§Ã£o de outliers em tempo real**, reduzindo necessidade de correÃ§Ãµes manuais.  
âœ… **IntegraÃ§Ã£o direta com Microsoft Teams**, facilitando a comunicaÃ§Ã£o da equipe.  
âœ… **FlexÃ­vel e escalÃ¡vel**, podendo ser ajustado para outros cenÃ¡rios.  

---

### ğŸš€ **Como Usar**  
1ï¸âƒ£ Configure um **webhook** do Microsoft Teams para receber as notificaÃ§Ãµes.  
2ï¸âƒ£ Adapte a **consulta SQL** conforme a base de dados utilizada.  
3ï¸âƒ£ Execute o cÃ³digo no **Databricks** ou outro ambiente compatÃ­vel com PySpark. 
